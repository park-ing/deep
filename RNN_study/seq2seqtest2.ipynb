{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seqtest2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wGqkEn0ewen"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import unicodedata\n",
        "import urllib3\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "http = urllib3.PoolManager()\n",
        "url = 'http://www.manythings.org/anki/fra-eng.zip'\n",
        "filename = 'fra-eng.zip'\n",
        "path = os.getcwd()\n",
        "zipfilename = os.path.join(path, filename)\n",
        "with http.request('GET', url, preload_content=False) as r, open(zipfilename, 'wb') as out_file:\n",
        "    shutil.copyfileobj(r, out_file)\n",
        "\n",
        "with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n",
        "    zip_ref.extractall(path)"
      ],
      "metadata": {
        "id": "Mjaq5QvgfGiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = 33000"
      ],
      "metadata": {
        "id": "_t3Y139zfOaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 함수들 구현\n",
        "def to_ascii(s):\n",
        "  # 프랑스어 악센트(accent) 삭제\n",
        "  # 예시 : 'déjà diné' -> deja dine\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "                   if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def preprocess_sentence(sent):\n",
        "  # 악센트 제거 함수 호출\n",
        "  sent = to_ascii(sent.lower())\n",
        "\n",
        "  # 단어와 구두점 사이에 공백 추가.\n",
        "  # ex) \"I am a student.\" => \"I am a student .\"\n",
        "  # re.sub('바꾸고싶은문자','바꿀문자',문자열이름,바꿀횟수)\n",
        "  sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n",
        "\n",
        "  # (a-z, A-Z, \".\", \"?\", \"!\", \",\") 이들을 제외하고는 전부 공백으로 변환.\n",
        "  sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
        "\n",
        "  # 다수 개의 공백을 하나의 공백으로 치환\n",
        "  sent = re.sub(r\"\\s+\", \" \", sent)\n",
        "  return sent"
      ],
      "metadata": {
        "id": "aDobvEa2fRSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 테스트\n",
        "en_sent = u\"Have you had dinner?\"\n",
        "fr_sent = u\"Avez-vous déjà diné?\"\n",
        "\n",
        "print('전처리 전 영어 문장 :', en_sent)\n",
        "print('전처리 후 영어 문장 :',preprocess_sentence(en_sent))\n",
        "print('전처리 전 프랑스어 문장 :', fr_sent)\n",
        "print('전처리 후 프랑스어 문장 :', preprocess_sentence(fr_sent))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bub-9xnwfw3b",
        "outputId": "aefdcb66-311e-4507-9d30-2ca5414eea01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전처리 전 영어 문장 : Have you had dinner?\n",
            "전처리 후 영어 문장 : have you had dinner ?\n",
            "전처리 전 프랑스어 문장 : Avez-vous déjà diné?\n",
            "전처리 후 프랑스어 문장 : avez vous deja dine ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_preprocessed_data():\n",
        "  encoder_input, decoder_input, decoder_target = [], [], []\n",
        "\n",
        "  with open(\"fra.txt\", \"r\") as lines:\n",
        "    for i, line in enumerate(lines):\n",
        "      # source 데이터와 target 데이터 분리\n",
        "      src_line, tar_line, _ = line.strip().split('\\t')\n",
        "\n",
        "      # source 데이터 전처리\n",
        "      src_line = [w for w in preprocess_sentence(src_line).split()]\n",
        "\n",
        "      # target 데이터 전처리\n",
        "      tar_line = preprocess_sentence(tar_line)\n",
        "      tar_line_in = [w for w in (\"<sos> \" + tar_line).split()]\n",
        "      tar_line_out = [w for w in (tar_line + \" <eos>\").split()]\n",
        "\n",
        "      encoder_input.append(src_line)\n",
        "      decoder_input.append(tar_line_in) # 훈련때 사용할 입력 시퀀스\n",
        "      decoder_target.append(tar_line_out) # 실제값, 디코더의 예측값과 비교\n",
        "\n",
        "      # 샘플 전체에 대하여 수행\n",
        "      if i == num_samples - 1:\n",
        "        break\n",
        "\n",
        "  return encoder_input, decoder_input, decoder_target"
      ],
      "metadata": {
        "id": "5QytZ-XSf7uF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sents_en_in, sents_fra_in, sents_fra_out = load_preprocessed_data()\n",
        "print('인코더의 입력 :',sents_en_in[:5])\n",
        "print('디코더의 입력 :',sents_fra_in[:5])\n",
        "print('디코더의 레이블 :',sents_fra_out[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K38fqE2Njyzo",
        "outputId": "b30072d1-3445-48c4-8844-00a9d82a43ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "인코더의 입력 : [['go', '.'], ['go', '.'], ['go', '.'], ['hi', '.'], ['hi', '.']]\n",
            "디코더의 입력 : [['<sos>', 'va', '!'], ['<sos>', 'marche', '.'], ['<sos>', 'bouge', '!'], ['<sos>', 'salut', '!'], ['<sos>', 'salut', '.']]\n",
            "디코더의 레이블 : [['va', '!', '<eos>'], ['marche', '.', '<eos>'], ['bouge', '!', '<eos>'], ['salut', '!', '<eos>'], ['salut', '.', '<eos>']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "의문점,\n",
        "현재 시점의 디코더 셀의 입력은 오직 이전 디코더 셀의 출력을 입력으로 받는데, sents_fra_in(decoder_input)이 필요한 이유.\n",
        "\n",
        "-> 훈련 과정은 이전 시점의 디코더 셀의 출력을 현재 시점의 디코더 셀의 입력으로 넣지 않고, 이전 시점의 실제값을 현재 시점의 디코더 셀의 입력값으로 하는 방법을 사용. (Teacher Forcing)\n"
      ],
      "metadata": {
        "id": "cqrhsMSkk0cv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 케라스 토크나이저를 통해 단어 집합을 생성, 정수 인코딩을 진행 후 이어서 패딩 진행\n",
        "# Tokenizer 예시 ['You', 'are', 'so', 'beautiful'] -> 1,5,30,99\n",
        "tokenizer_en = Tokenizer(filters=\"\", lower=False)\n",
        "tokenizer_en.fit_on_texts(sents_en_in)\n",
        "encoder_input = tokenizer_en.texts_to_sequences(sents_en_in)\n",
        "encoder_input = pad_sequences(encoder_input, padding=\"post\")  # data set내의 문장 길이가 모두 다르기 때문에 모두 같은 길이로 맞춰 주는 작업(pre-padding)\n",
        "\n",
        "tokenizer_fra = Tokenizer(filters=\"\", lower=False)\n",
        "tokenizer_fra.fit_on_texts(sents_fra_in)\n",
        "tokenizer_fra.fit_on_texts(sents_fra_out)\n",
        "\n",
        "decoder_input = tokenizer_fra.texts_to_sequences(sents_fra_in)\n",
        "decoder_input = pad_sequences(decoder_input, padding=\"post\")\n",
        "\n",
        "decoder_target = tokenizer_fra.texts_to_sequences(sents_fra_out)\n",
        "decoder_target = pad_sequences(decoder_target, padding=\"post\")"
      ],
      "metadata": {
        "id": "2dp__FUYk350"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터의 크기(shape)를 확인\n",
        "print('인코더의 입력의 크기(shape) :',encoder_input.shape)\n",
        "print('디코더의 입력의 크기(shape) :',decoder_input.shape)\n",
        "print('디코더의 레이블의 크기(shape) :',decoder_target.shape)\n",
        "# 영문장 길이 8, 프랑스어 문장 길어 16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-erVIyWnKm0",
        "outputId": "fe010606-377d-4479-92f0-106e9988d0a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "인코더의 입력의 크기(shape) : (33000, 8)\n",
            "디코더의 입력의 크기(shape) : (33000, 16)\n",
            "디코더의 레이블의 크기(shape) : (33000, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 집합의 크기\n",
        "src_vocab_size = len(tokenizer_en.word_index) + 1\n",
        "tar_vocab_size = len(tokenizer_fra.word_index) + 1\n",
        "print(\"영어 단어 집합의 크기 : {:d}, 프랑스어 단어 집합의 크기 : {:d}\".format(src_vocab_size, tar_vocab_size))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xg1TsCMvnvXZ",
        "outputId": "fe98606b-56c7-4b63-dfe2-91fff33581e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "영어 단어 집합의 크기 : 4637, 프랑스어 단어 집합의 크기 : 8115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어-> 정수 / 정수 -> 단어를 얻는 딕셔너리를 각각 생성\n",
        "# 이들은 훈련을 마치고 예측값과 실제값을 비교하는 단계에서 사용됨.\n",
        "src_to_index = tokenizer_en.word_index\n",
        "index_to_src = tokenizer_en.index_word\n",
        "tar_to_index = tokenizer_fra.word_index\n",
        "index_to_tar = tokenizer_fra.index_word\n"
      ],
      "metadata": {
        "id": "hgYBJrwhoewh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 셔플\n",
        "indices = np.arange(encoder_input.shape[0]) # 33000\n",
        "np.random.shuffle(indices)\n",
        "print('랜덤 시퀀스 :',indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-gzXb9tozm5",
        "outputId": "e59db4e0-6be4-4fb5-f8d2-cecf8229a886"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "랜덤 시퀀스 : [19701 11995 28913 ...  1701 28816   107]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 샘플을 기존 순서와 다르게 섞는다.\n",
        "encoder_input = encoder_input[indices]\n",
        "decoder_input = decoder_input[indices]\n",
        "decoder_target = decoder_target[indices]"
      ],
      "metadata": {
        "id": "J7RDRkTeo7ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 임의로 30,997번째 샘플을 출력해본다.\n",
        "encoder_input[30997]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gg70FQLZpDam",
        "outputId": "442eaa51-d42e-49d2-ffb0-f352a6640d44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3432,   22,  148,   10,    3,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_input[30997]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntjpd3XrpyoQ",
        "outputId": "1e4bf5a5-40bc-4e3e-e261-df0976394f7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   2, 3432,   22,  148,   10,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_target[30997]\n",
        "# input과 target에서 데이터 구조상 앞에 붙은 <sos>토큰과 뒤에 붙은 <eos>을 제외하면 동일한 정수 시퀀스이어야 함."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5v5ehd_7pzsd",
        "outputId": "ec61413d-e70f-4a35-8b9a-c6723ee7c965"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3432,   22,  148,   10,    3,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 데이터의 10%를 테스트 데이터로 분리\n",
        "n_of_val = int(33000*0.1)\n",
        "print('검증 데이터의 개수 :',n_of_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUDEuGhPq8cO",
        "outputId": "55466a83-a7c5-4568-bcf9-7e48f6067a5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "검증 데이터의 개수 : 3300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 33,000개의 10%에 해당되는 3,300개 데이터를 test data로 사용\n",
        "encoder_input_train = encoder_input[:-n_of_val]\n",
        "decoder_input_train = decoder_input[:-n_of_val]\n",
        "decoder_target_train = decoder_target[:-n_of_val]\n",
        "\n",
        "encoder_input_test = encoder_input[-n_of_val:]\n",
        "decoder_input_test = decoder_input[-n_of_val:]\n",
        "decoder_target_test = decoder_target[-n_of_val:]"
      ],
      "metadata": {
        "id": "6fjQkU65rC4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('훈련 source 데이터의 크기 :',encoder_input_train.shape)\n",
        "print('훈련 target 데이터의 크기 :',decoder_input_train.shape)\n",
        "print('훈련 target 레이블의 크기 :',decoder_target_train.shape)\n",
        "print('테스트 source 데이터의 크기 :',encoder_input_test.shape)\n",
        "print('테스트 target 데이터의 크기 :',decoder_input_test.shape)\n",
        "print('테스트 target 레이블의 크기 :',decoder_target_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tj5AaUu2rNyZ",
        "outputId": "afbfcc1d-7397-440e-ac11-7f1b147d8175"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 source 데이터의 크기 : (29700, 8)\n",
            "훈련 target 데이터의 크기 : (29700, 16)\n",
            "훈련 target 레이블의 크기 : (29700, 16)\n",
            "테스트 source 데이터의 크기 : (3300, 8)\n",
            "테스트 target 데이터의 크기 : (3300, 16)\n",
            "테스트 target 레이블의 크기 : (3300, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "기계 번역기 만들기"
      ],
      "metadata": {
        "id": "KvjVpGVxrwlF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n",
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "id": "_qwMeEWhrveY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 임베딩 벡터 차원\n",
        "embedding_dim = 64\n",
        "# LSTM의 은닉 상태 크기  \n",
        "hidden_units = 64"
      ],
      "metadata": {
        "id": "j0479HT8r1gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM에서 state_h, state_c를 리턴받는데, 이는 각각 RNN 챕터에서 LSTM을 처음 설명할 때 언급하였던 은닉 상태와 셀 상태에 해당. 이 두 가지 상태를 encoder_states에 저장. encoder_states를 디코더에 전달하므로서 이 두 가지 상태 모두를 디코더로 전달할 예정. 이것이 앞서 배운 컨텍스트 벡터다."
      ],
      "metadata": {
        "id": "91Z7N0nEsi3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 인코더\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "enc_emb = Embedding(src_vocab_size, embedding_dim)(encoder_inputs) # 임베딩 층\n",
        "enc_masking = Masking(mask_value=0.0)(enc_emb) # 패딩 0은 연산에서 제외\n",
        "encoder_lstm = LSTM(hidden_units, return_state=True) # 상태값 리턴을 위해 return_state는 True / 인코더의 내부 상태를 디코더로 넘겨줘야 한다.\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(enc_masking) # 은닉 상태와 셀 상태를 리턴\n",
        "encoder_states = [state_h, state_c] # 인코더의 은닉 상태와 셀 상태를 저장"
      ],
      "metadata": {
        "id": "x8shvRHEsB0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "디코더는 인코더의 마지막 은닉 상태로부터 초기 은닉 상태를 얻는다. initial_state의 인자값으로 encoder_states를 주는 코드가 이에 해당됨\u001d. 디코더도 은닉 상태, 셀 상태를 리턴하기는 하지만 훈련 과정에서는 사용하지 않는다. 다중 클래스 분류 문제이므로 출력층으로 소프트맥스 함수와 손실 함수를 크로스 엔트로피 함수를 사용합니다.\n",
        "\n",
        "categorical_crossentropy를 사용하려면 레이블은 원-핫 인코딩이 된 상태여야 한다. 그런데 현재 decoder_outputs의 경우에는 원-핫 인코딩을 하지 않은 상태이다. 원-핫 인코딩을 하지 않은 상태로 정수 레이블에 대해서 다중 클래스 분류 문제를 풀고자 하는 경우에는 categorical_crossentropy가 아니라 sparse_categorical_crossentropy를 사용하면 됨."
      ],
      "metadata": {
        "id": "4r5dUFY6thro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 디코더\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "dec_emb_layer = Embedding(tar_vocab_size, hidden_units) # 임베딩 층\n",
        "dec_emb = dec_emb_layer(decoder_inputs) # 패딩 0은 연산에서 제외\n",
        "dec_masking = Masking(mask_value=0.0)(dec_emb)\n",
        "\n",
        "# 상태값 리턴을 위해 return_state는 True, 모든 시점에 대해서 단어를 예측하기 위해 return_sequences는 True\n",
        "decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True) \n",
        "\n",
        "# 인코더의 은닉 상태를 초기 은닉 상태(initial_state)로 사용\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_masking,\n",
        "                                     initial_state=encoder_states)\n",
        "\n",
        "# 모든 시점의 결과에 대해서 소프트맥스 함수를 사용한 출력층을 통해 단어 예측\n",
        "decoder_dense = Dense(tar_vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# 모델의 입력과 출력을 정의.\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])"
      ],
      "metadata": {
        "id": "ln7sVq6muQKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 128배치크기, 총 50 에포크로 학습\n",
        "model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
        "          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),\n",
        "          batch_size=128, epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2FrDMMWurHq",
        "outputId": "42859776-5f5f-432d-e562-a82c205c923d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "233/233 [==============================] - 25s 51ms/step - loss: 3.4011 - acc: 0.6119 - val_loss: 1.9870 - val_acc: 0.6218\n",
            "Epoch 2/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 1.8331 - acc: 0.6909 - val_loss: 1.7347 - val_acc: 0.7381\n",
            "Epoch 3/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 1.6405 - acc: 0.7453 - val_loss: 1.5943 - val_acc: 0.7534\n",
            "Epoch 4/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 1.5145 - acc: 0.7570 - val_loss: 1.4874 - val_acc: 0.7622\n",
            "Epoch 5/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 1.4169 - acc: 0.7709 - val_loss: 1.4130 - val_acc: 0.7747\n",
            "Epoch 6/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 1.3393 - acc: 0.7793 - val_loss: 1.3454 - val_acc: 0.7808\n",
            "Epoch 7/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 1.2667 - acc: 0.7879 - val_loss: 1.2836 - val_acc: 0.7898\n",
            "Epoch 8/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 1.2036 - acc: 0.7989 - val_loss: 1.2333 - val_acc: 0.8031\n",
            "Epoch 9/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 1.1455 - acc: 0.8098 - val_loss: 1.1843 - val_acc: 0.8082\n",
            "Epoch 10/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 1.0917 - acc: 0.8159 - val_loss: 1.1401 - val_acc: 0.8143\n",
            "Epoch 11/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 1.0432 - acc: 0.8228 - val_loss: 1.1001 - val_acc: 0.8207\n",
            "Epoch 12/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 0.9979 - acc: 0.8293 - val_loss: 1.0667 - val_acc: 0.8248\n",
            "Epoch 13/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 0.9576 - acc: 0.8347 - val_loss: 1.0386 - val_acc: 0.8293\n",
            "Epoch 14/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 0.9197 - acc: 0.8397 - val_loss: 1.0123 - val_acc: 0.8327\n",
            "Epoch 15/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 0.8854 - acc: 0.8434 - val_loss: 0.9910 - val_acc: 0.8351\n",
            "Epoch 16/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 0.8542 - acc: 0.8467 - val_loss: 0.9696 - val_acc: 0.8375\n",
            "Epoch 17/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 0.8242 - acc: 0.8497 - val_loss: 0.9532 - val_acc: 0.8395\n",
            "Epoch 18/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 0.7964 - acc: 0.8525 - val_loss: 0.9349 - val_acc: 0.8420\n",
            "Epoch 19/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 0.7703 - acc: 0.8551 - val_loss: 0.9191 - val_acc: 0.8427\n",
            "Epoch 20/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 0.7458 - acc: 0.8576 - val_loss: 0.9046 - val_acc: 0.8446\n",
            "Epoch 21/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 0.7219 - acc: 0.8605 - val_loss: 0.8936 - val_acc: 0.8457\n",
            "Epoch 22/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 0.6996 - acc: 0.8628 - val_loss: 0.8825 - val_acc: 0.8470\n",
            "Epoch 23/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 0.6783 - acc: 0.8654 - val_loss: 0.8714 - val_acc: 0.8472\n",
            "Epoch 24/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 0.6573 - acc: 0.8678 - val_loss: 0.8595 - val_acc: 0.8490\n",
            "Epoch 25/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 0.6374 - acc: 0.8700 - val_loss: 0.8516 - val_acc: 0.8496\n",
            "Epoch 26/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 0.6185 - acc: 0.8720 - val_loss: 0.8401 - val_acc: 0.8525\n",
            "Epoch 27/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 0.5998 - acc: 0.8745 - val_loss: 0.8338 - val_acc: 0.8524\n",
            "Epoch 28/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 0.5821 - acc: 0.8770 - val_loss: 0.8256 - val_acc: 0.8534\n",
            "Epoch 29/50\n",
            "233/233 [==============================] - 9s 38ms/step - loss: 0.5648 - acc: 0.8791 - val_loss: 0.8185 - val_acc: 0.8551\n",
            "Epoch 30/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 0.5490 - acc: 0.8813 - val_loss: 0.8152 - val_acc: 0.8552\n",
            "Epoch 31/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 0.5332 - acc: 0.8838 - val_loss: 0.8058 - val_acc: 0.8570\n",
            "Epoch 32/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 0.5173 - acc: 0.8861 - val_loss: 0.8011 - val_acc: 0.8572\n",
            "Epoch 33/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 0.5032 - acc: 0.8883 - val_loss: 0.7983 - val_acc: 0.8580\n",
            "Epoch 34/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 0.4887 - acc: 0.8906 - val_loss: 0.7925 - val_acc: 0.8587\n",
            "Epoch 35/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 0.4763 - acc: 0.8928 - val_loss: 0.7862 - val_acc: 0.8602\n",
            "Epoch 36/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 0.4614 - acc: 0.8951 - val_loss: 0.7810 - val_acc: 0.8609\n",
            "Epoch 37/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 0.4488 - acc: 0.8973 - val_loss: 0.7793 - val_acc: 0.8609\n",
            "Epoch 38/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 0.4390 - acc: 0.8987 - val_loss: 0.7743 - val_acc: 0.8618\n",
            "Epoch 39/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 0.4257 - acc: 0.9012 - val_loss: 0.7708 - val_acc: 0.8631\n",
            "Epoch 40/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 0.4129 - acc: 0.9038 - val_loss: 0.7680 - val_acc: 0.8628\n",
            "Epoch 41/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 0.4023 - acc: 0.9057 - val_loss: 0.7660 - val_acc: 0.8641\n",
            "Epoch 42/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 0.3923 - acc: 0.9076 - val_loss: 0.7639 - val_acc: 0.8646\n",
            "Epoch 43/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 0.3828 - acc: 0.9093 - val_loss: 0.7622 - val_acc: 0.8650\n",
            "Epoch 44/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 0.3725 - acc: 0.9114 - val_loss: 0.7591 - val_acc: 0.8652\n",
            "Epoch 45/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 0.3629 - acc: 0.9133 - val_loss: 0.7597 - val_acc: 0.8652\n",
            "Epoch 46/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 0.3536 - acc: 0.9153 - val_loss: 0.7569 - val_acc: 0.8663\n",
            "Epoch 47/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 0.3446 - acc: 0.9169 - val_loss: 0.7551 - val_acc: 0.8668\n",
            "Epoch 48/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 0.3373 - acc: 0.9184 - val_loss: 0.7541 - val_acc: 0.8669\n",
            "Epoch 49/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 0.3295 - acc: 0.9197 - val_loss: 0.7511 - val_acc: 0.8686\n",
            "Epoch 50/50\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 0.3210 - acc: 0.9217 - val_loss: 0.7505 - val_acc: 0.8683\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f899795e1d0>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "seq2seq 기계 번역기 동작시키기\n",
        "\n",
        "seq2seq는 훈련 과정(교사 강요)과 테스트 과정에서의 동작 방식이 다르다. 그래서 테스트 과정을 위해 모델을 다시 설계해주어야 한다. 특히 디코더를 수정해야 한다. 번역 단계를 위해 모델을 수정하고 동작.\n",
        "\n",
        "전체적인 번역 단계를 정리하면 아래와 같습니다.\n",
        "\n",
        "1. 번역하고자 하는 입력 문장이 인코더로 입력되어 인코더의 마지막 시점의 은닉 상태와 셀 상태를 얻습니다.\n",
        "2. 인코더의 은닉 상태와 셀 상태, 그리고 토큰 <sos>를 디코더로 보냅니다.\n",
        "3. 디코더가 토큰 <eos>가 나올 때까지 다음 단어를 예측하는 행동을 반복합니다.\n",
        "\n",
        "인코더의 입, 출력으로 사용하는 encoder_inputs와 encoder_states는 훈련 과정에서 이미 정의한 것들을 재사용한다.\n",
        "\n",
        "이렇게 되면 훈련 단계에 encoder_inputs와 encoder_states 사이에 있는 모든 층까지 전부 불러오게 되므로 결과적으로 훈련 단계에서 사용한 인코더를 그대로 재사용하게 된다. \n",
        "\n",
        "이어서 디코더를 설계한다. 테스트 단계에서는 디코더를 매 시점 별로 컨트롤 할 예정으로, 이를 위해서 이전 시점의 상태를 저장할 텐서인 decoder_state_input_h, decoder_state_input_c를 정의. 매 시점 별로 디코더를 컨트롤하는 함수는 뒤에서 정의할 decode_sequence()로 해당 함수를 자세히 살펴봐야 함."
      ],
      "metadata": {
        "id": "n5o1OMy6xU4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 인코더\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# 디코더 설계 시작\n",
        "# 이전 시점의 상태를 보관할 텐서\n",
        "decoder_state_input_h = Input(shape=(hidden_units,))\n",
        "decoder_state_input_c = Input(shape=(hidden_units,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "# 훈련 때 사용했던 임베딩 층을 재사용\n",
        "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# 다음 단어 예측을 위해 이전 시점의 상태를 현 시점의 초기 상태로 사용\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
        "decoder_states2 = [state_h2, state_c2]\n",
        "\n",
        "# 모든 시점에 대해서 단어 예측\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "\n",
        "# 수정된 디코더\n",
        "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs2] + decoder_states2)"
      ],
      "metadata": {
        "id": "Xeit3GsixUJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "테스트 단계에서의 동작을 위한 decode_sequence 함수를 구현. \n",
        "\n",
        "입력 문장이 들어오면 인코더는 마지막 시점까지 전개하여 마지막 시점의 은닉 상태와 셀 상태를 리턴한다. 이 두 개의 값을 states_value에 저장. 그리고 디코더의 초기 입력으로 <SOS>를 준비해서 이를 target_seq에 저장한다. \n",
        "\n",
        "이 두 가지 입력을 가지고 while문 안으로 진입하여 이 두 가지를 디코더의 입력으로 사용한다. 이제 디코더는 현재 시점에 대해서 예측을 하게 되는데, 현재 시점의 예측 벡터가 output_tokens, 현재 시점의 은닉 상태가 h, 현재 시점의 셀 상태가 c이다. 예측 벡터로부터 현재 시점의 예측 단어인 target_seq를 얻고, h와 c 이 두 개의 값은 states_value에 저장한다. 그리고 while문의 다음 루프. 즉, 두번째 시점의 디코더의 입력으로 다시 target_seq와 states_value를 사용한다. \n",
        "\n",
        "이를 현재 시점의 예측 단어로 <eos>를 예측하거나 번역 문장의 길이가 50이 넘는 순간까지 반복. 각 시점마다 번역된 다어는 decoded_sentence에 누적하여 저장하였다가 최종 번역 시퀀스로 리턴."
      ],
      "metadata": {
        "id": "fpmP_QZGyj-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq):\n",
        "  # 입력으로부터 인코더의 마지막 시점의 상태(은닉 상태, 셀 상태)를 얻음\n",
        "  states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "  # <SOS>에 해당하는 정수 생성\n",
        "  target_seq = np.zeros((1,1))\n",
        "  target_seq[0, 0] = tar_to_index['<sos>']\n",
        "\n",
        "  stop_condition = False\n",
        "  decoded_sentence = ''\n",
        "\n",
        "  # stop_condition이 True가 될 때까지 루프 반복\n",
        "  # 구현의 간소화를 위해서 이 함수는 배치 크기를 1로 가정합니다.\n",
        "  while not stop_condition:\n",
        "    # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
        "    output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "    # 예측 결과를 단어로 변환\n",
        "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "    sampled_char = index_to_tar[sampled_token_index]\n",
        "\n",
        "    # 현재 시점의 예측 단어를 예측 문장에 추가\n",
        "    decoded_sentence += ' '+sampled_char\n",
        "\n",
        "    # <eos>에 도달하거나 정해진 길이를 넘으면 중단.\n",
        "    if (sampled_char == '<eos>' or\n",
        "        len(decoded_sentence) > 50):\n",
        "        stop_condition = True\n",
        "\n",
        "    # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
        "    target_seq = np.zeros((1,1))\n",
        "    target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "    # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
        "    states_value = [h, c]\n",
        "\n",
        "  return decoded_sentence"
      ],
      "metadata": {
        "id": "6B9IXAJX0TzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "결과 확인을 위한 함수를 만든다.\n",
        "\n",
        "seq_to_src 함수는 영어 문장에 해당하는 정수 시퀀스를 입력받으면 정수로부터 영어 단어를 리턴하는 index_to_src를 통해 영어 문장으로 변환한다. \n",
        "\n",
        "seq_to_tar은 프랑스어에 해당하는 정수 시퀀스를 입력받으면 정수로부터 프랑스어 단어를 리턴하는 index_to_tar을 통해 프랑스어 문장으로 변환."
      ],
      "metadata": {
        "id": "jZrgzDhxJGpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
        "def seq_to_src(input_seq):\n",
        "  sentence = ''\n",
        "  for encoded_word in input_seq:\n",
        "    if(encoded_word != 0):\n",
        "      sentence = sentence + index_to_src[encoded_word] + ' '\n",
        "  return sentence\n",
        "\n",
        "# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
        "def seq_to_tar(input_seq):\n",
        "  sentence = ''\n",
        "  for encoded_word in input_seq:\n",
        "    if(encoded_word != 0 and encoded_word != tar_to_index['<sos>'] and encoded_word != tar_to_index['<eos>']):\n",
        "      sentence = sentence + index_to_tar[encoded_word] + ' '\n",
        "  return sentence"
      ],
      "metadata": {
        "id": "4ZZBbLcJ0WIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 결과 샘플 출력\n",
        "for seq_index in [3, 50, 100, 300, 1001]:\n",
        "  input_seq = encoder_input_train[seq_index: seq_index + 1]\n",
        "  decoded_sentence = decode_sequence(input_seq)\n",
        "\n",
        "  print(\"입력문장 :\",seq_to_src(encoder_input_train[seq_index]))\n",
        "  print(\"정답문장 :\",seq_to_tar(decoder_input_train[seq_index]))\n",
        "  print(\"번역문장 :\",decoded_sentence[1:-5])\n",
        "  print(\"-\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9QLqTmp0YSb",
        "outputId": "4e80fe8b-46d0-4497-841d-08ee7dccfa77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력문장 : i m the youngest . \n",
            "정답문장 : je suis le plus jeune . \n",
            "번역문장 : je suis le plus jeune . \n",
            "--------------------------------------------------\n",
            "입력문장 : i signed the check . \n",
            "정답문장 : j ai signe le cheque . \n",
            "번역문장 : j ai apporte la voiture . \n",
            "--------------------------------------------------\n",
            "입력문장 : you re a loser . \n",
            "정답문장 : tu es un minable . \n",
            "번역문장 : tu es un bon . \n",
            "--------------------------------------------------\n",
            "입력문장 : allow me to go . \n",
            "정답문장 : permettez moi d y aller ! \n",
            "번역문장 : permettez moi d y aller ! \n",
            "--------------------------------------------------\n",
            "입력문장 : we re reliable . \n",
            "정답문장 : nous sommes fiables . \n",
            "번역문장 : nous sommes fiables . \n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test data에 대해서도 샘플 결과 출력\n",
        "for seq_index in [3, 50, 100, 300, 1001]:\n",
        "  input_seq = encoder_input_test[seq_index: seq_index + 1]\n",
        "  decoded_sentence = decode_sequence(input_seq)\n",
        "\n",
        "  print(\"입력문장 :\",seq_to_src(encoder_input_test[seq_index]))\n",
        "  print(\"정답문장 :\",seq_to_tar(decoder_input_test[seq_index]))\n",
        "  print(\"번역문장 :\",decoded_sentence[1:-5])\n",
        "  print(\"-\"*50)"
      ],
      "metadata": {
        "id": "HyXG9Ui50jKm",
        "outputId": "a936d9d8-aa6c-4a22-a366-4ec6cf338fc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력문장 : say hello . \n",
            "정답문장 : dis bonjour . \n",
            "번역문장 : bonjour ! \n",
            "--------------------------------------------------\n",
            "입력문장 : here is your book . \n",
            "정답문장 : voici ton livre . \n",
            "번역문장 : voici votre livre . \n",
            "--------------------------------------------------\n",
            "입력문장 : how big is he ? \n",
            "정답문장 : il est grand comment ? \n",
            "번역문장 : comment a t il ? \n",
            "--------------------------------------------------\n",
            "입력문장 : the room is empty . \n",
            "정답문장 : la piece est vide . \n",
            "번역문장 : la chambre est vide . \n",
            "--------------------------------------------------\n",
            "입력문장 : what a dope ! \n",
            "정답문장 : quel couillon ! \n",
            "번역문장 : quelle andouille ! \n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}