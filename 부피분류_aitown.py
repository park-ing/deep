# -*- coding: utf-8 -*-
"""20210310부피_AITOWN.ipynb의 사본

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F0IkLgOoXYkqyGq4jZxqt0IRNBK3Rq4-
"""

import os
from PIL import Image
import tensorflow as tf
import cv2 as cv
from tensorflow import keras
from matplotlib import image
from matplotlib import pyplot as plt
import torch 
import pandas as pd
from torch.utils.data import Dataset, DataLoader
import numpy as np
import torchvision
from torchvision import transforms
import random
from glob import glob
import gc

pip install tensorflow==2.4.1

print(tf.__version__)
print(keras.__version__)

from google.colab import drive
drive.mount('/content/drive')

volume0_list = glob('/content/drive/My Drive/volume_2021/volume0/*.jpg')
volume1_list = glob('/content/drive/My Drive/volume_2021/volume1/*.jpg')
volume2_list = glob('/content/drive/My Drive/volume_2021/volume2/*.jpg')

balance = min(len(volume0_list),len(volume1_list),len(volume2_list))
# balance = 3

balance*3

index_0 = []
index_1 = []
index_2 = []
for i in range((balance)):
  # if i not in index_0
  index_0.append(volume0_list[random.randint(0,len(volume0_list)-1)])
  index_1.append(volume1_list[random.randint(0,len(volume1_list)-1)])
  index_2.append(volume2_list[random.randint(0,len(volume2_list)-1)])

def img_to_np(fpaths,label,m,n,resize=True):  
    img_array = []
    lab_array = []
    for fname in fpaths:
      try:
        # print(fname)
        img = Image.open(fname).convert('RGB')
        if(resize): img = img.resize((m, n))
        img_array.append(np.asarray(img))
        lab_array.append(label)
      except:
        continue
    images = np.array(img_array)
    labels = np.array(lab_array)
    return images, labels

img_volume0,lab_volume0 = img_to_np(index_0,0,224,224)
img_volume0 = img_volume0.astype(np.float32)/ 255.

img_volume1,lab_volume1 = img_to_np(index_1,1,224,224)
img_volume1 = img_volume1.astype(np.float32)/ 255.

img_volume2,lab_volume2 = img_to_np(index_2,2,224,224)
img_volume2 = img_volume2.astype(np.float32)/ 255.

def noisy(noise_typ,image):
   if noise_typ == "gauss":
      row,col,ch= image.shape
      mean = 0
      var = 0.1
      sigma = var**0.5
      gauss = np.random.normal(mean,sigma,(row,col,ch))
      gauss = gauss.reshape(row,col,ch)
      noisy = image + gauss
      return noisy
      
   elif noise_typ == "s":
      row,col,ch = image.shape
      s_vs_p = 0.5
      amount = 0.004
      out = np.copy(image)
      # Salt mode
      num_salt = np.ceil(amount * image.size * s_vs_p)
      coords = [np.random.randint(0, i - 1, int(num_salt)) for i in image.shape]
      out[coords] = 1
      return out

   elif noise_typ =="p":
      row,col,ch = image.shape
      s_vs_p = 0.5
      amount = 0.004
      out = np.copy(image)
      # Pepper mode
      num_pepper = np.ceil(amount* image.size * (1. - s_vs_p))
      coords = [np.random.randint(0, i - 1, int(num_pepper))for i in image.shape]
      out[coords] = 0
      return out

   elif noise_typ == "poisson":
      vals = len(np.unique(image))
      vals = 2 ** np.ceil(np.log2(vals))
      noisy = np.random.poisson(image * vals)
      return noisy

   elif noise_typ =="speckle":
      row,col,ch = image.shape
      gauss = np.random.randn(row,col,ch)
      gauss = gauss.reshape(row,col,ch)        
      noisy = image + image * gauss
      return noisy

def make_image_array(IMG,label):
  # m,n = 224,224
  imagess=[]
  image_label=[]
  #IMG=np.asarray(IMG)
  for i in range(len(IMG)):
    img=IMG[i]
    flipped = tf.image.flip_left_right(img)
    rotated90 = tf.image.rot90(img)
    rotated180 = tf.image.rot90(rotated90)
    # rotated270 = tf.image.rot90(rotated180)
    gauss = noisy('gauss',img)
    # s = noisy('s',img)
    p = noisy('p',img)
    # poisson = noisy('poisson',img)
    speckel = noisy('speckle',img)
    # cropped = tf.image.central_crop(img, central_fraction=0.85)
    # cropped1 = tf.image.central_crop(img, central_fraction=0.9)
    # cropped2 = tf.image.central_crop(img, central_fraction=0.95)
    #grayscaled = tf.image.rgb_to_grayscale(img)
    # saturated = tf.image.adjust_saturation(img, 3)
    saturated1 = tf.image.adjust_saturation(img, 2)
    # bright = tf.image.adjust_brightness(img, 0.4)
    bright1 = tf.image.adjust_brightness(img, 0.1)
    # bright2 = tf.image.adjust_brightness(img, 0.8)
    
    # img=cv.cvtColor(img,cv.COLOR_BGR2GRAY)
    # flipped=cv.cvtColor(np.float32(flipped),cv.COLOR_BGR2GRAY)
    # rotated90 = cv.cvtColor(np.float32(rotated90),cv.COLOR_BGR2GRAY)
    # rotated180 = cv.cvtColor(np.float32(rotated180),cv.COLOR_BGR2GRAY)
    # rotated270 = cv.cvtColor(np.float32(rotated270),cv.COLOR_BGR2GRAY)
    # cropped = cv.cvtColor(np.float32(cropped),cv.COLOR_BGR2GRAY)
    # cropped1 = cv.cvtColor(np.float32(cropped1),cv.COLOR_BGR2GRAY)
    # cropped2 = cv.cvtColor(np.float32(cropped2),cv.COLOR_BGR2GRAY)    
    
    # cropped=cv.resize(np.float32(cropped),(m,n), interpolation=cv.INTER_AREA)
    # cropped1=cv.resize(np.float32(cropped1),(m,n), interpolation=cv.INTER_AREA)
    # cropped2=cv.resize(np.float32(cropped2),(m,n), interpolation=cv.INTER_AREA)

    imagess.append(np.float32(img))
    imagess.append(np.float32(flipped))
    # imagess.append(np.float32(rotated90))
    imagess.append(np.float32(rotated180))
    # imagess.append(np.float32(rotated270))
    imagess.append(gauss)
    # imagess.append(s)
    # imagess.append(p)
    # imagess.append(poisson)
    imagess.append(speckel)
    # imagess.append(np.float32(cropped))
    # imagess.append(np.float32(cropped1))
    # imagess.append(np.float32(cropped2))
    #images.append(grayscaled)
    # imagess.append(np.float32(saturated))
    imagess.append(np.float32(saturated1))
    # imagess.append(np.float32(bright))
    imagess.append(np.float32(bright1))
    # imagess.append(np.float32(bright2))
    
    l = label[i]
    for j in range(7):
      image_label.append(l)

  
  return np.asarray(imagess), np.asarray(image_label)

def Top_K_accuracy_for_k_fold(model,Test_images,Test_labels,j,history):
  top1 = 0.0
  top2 = 0.0
  top3 = 0.0
  # top4 = 0.0
  # top5 = 0.0    
  class_probs = model.predict(Test_images)
  for i, l in enumerate(Test_labels):
      class_prob = class_probs[i]
      
      top_values1 = (-class_prob).argsort()[:1]
      top_values2 = (-class_prob).argsort()[:2]
      top_values3 = (-class_prob).argsort()[:3]
      # top_values4 = (-class_prob).argsort()[:4]
      # top_values5 = (-class_prob).argsort()[:5]
      # 
      if np.isin(np.array([l]), top_values1):
          top1 += 1.0
      if np.isin(np.array([l]), top_values2):
          top2 += 1.0    
      if np.isin(np.array([l]), top_values3):
          top3 += 1.0
      # if np.isin(np.array([l]), top_values4):
      #     top4 += 1.0    
      # if np.isin(np.array([l]), top_values5):
          # top5 += 1.0

  TT1=round((1-top1/len(Test_labels))*100,1)
  TT2=round((1-top2/len(Test_labels))*100,1)
  TT3=round((1-top3/len(Test_labels))*100,1)
  # TT4=round((1-top4/len(Test_labels))*100,1)
  # TT5=round((1-top5/len(Test_labels))*100,1)

  return TT1,TT2,TT3

def make_test_array(IMG,label):
  imagess=[]
  image_label=[]
  for i in range(len(IMG)):
    img=IMG[i]
    imagess.append(np.float32(img))
    
  # for i in range(len(imagess)):
  #   image_label.append(label)
  return np.asarray(imagess), np.asarray(image_label)

class_names=['green','yellow','red']
num_class = len(class_names)

from sklearn.utils import shuffle
Meta_image = np.concatenate((img_volume0, img_volume1, img_volume2), axis=0)
Meta_label = np.concatenate((lab_volume0, lab_volume1, lab_volume2), axis=0)
Meta_image, Meta_label = shuffle(Meta_image, Meta_label)

train_img, train_lab = make_image_array(Meta_image,Meta_label)

QQ=[]
for i in range(7):
  QQ.append(train_img[i])

fig, axes = plt.subplots(1,7, figsize=(80,80))
axes = axes.flatten()
for i in range(len(QQ)):
   axes[i].imshow(QQ[i])
   axes[i].axis('off')
plt.show()

plt.imshow(train_img[0])
plt.axis('off')

from numpy.random import seed
from sklearn.model_selection import KFold

seed(1337)

S=100

num_folds = 5
# inputs = np.concatenate((Train_images1, Test_images1, Valid_images1), axis=0)
# targets = np.concatenate((Train_labels, Test_labels, Valid_labels), axis=0)


# Define per-fold score containers
d121_t1=[]
d121_t2=[]
d121_t3=[]
H=[]
d121_acc_per_fold = []
d121_loss_per_fold = []

# Define the K-fold Cross Validator
kfold = KFold(n_splits=num_folds, shuffle=True)

# K-fold Cross Validation model evaluation
batch = 64

fold_no = 1

for train, test in kfold.split(Meta_image, Meta_label):
  # Model configuration
  step =S
  lr = 1e-3
  #Opt = tf.keras.optimizers.Adam(L)
  Opt = tf.keras.optimizers.SGD(learning_rate = 0.001,momentum = 0.9)
  Loss= tf.keras.losses.SparseCategoricalCrossentropy()
  dir_path = '/content/drive/My Drive/CleanHouse_volume_2021/'+str(fold_no)
  # path = dir_path+'/cp.ckpt'
  path = dir_path
  #@@@@@@@@@
  train_img,train_lab = Meta_image[train], Meta_label[train]
  # train_img, train_lab = make_image_array(Meta_image[train],Meta_label[train])
  # train_img = train_img[:,:,0]
  va = test[0:int(len(test)/2)]
  te = test[int(len(test)/2):]
  # train_img, train_lab = Meta_image[train],Meta_label[train]
  valid_img, valid_lab = Meta_image[va],Meta_label[va]
  test_img, test_lab = Meta_image[te],Meta_label[te]

  # Define the model architecture
  model = tf.keras.Sequential([
  tf.keras.applications.DenseNet121(weights='imagenet',input_shape=(224,224,3),pooling=None,classes=1000),
  tf.keras.layers.Dense(100,activation='relu'),
  tf.keras.layers.Dense(num_class,activation='softmax',activity_regularizer = tf.keras.regularizers.l2(1e-5))
  ])
  
  model.compile(optimizer=Opt,loss=Loss,metrics=['accuracy'])
  reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.8,patience=int(S/20), min_lr=1e-10,verbose=1)
  es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=int(5))
  mc = tf.keras.callbacks.ModelCheckpoint(path, monitor='val_accuracy', mode='auto', verbose=1,save_weights_only=False ,save_best_only=True)
  #callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=4)

  # Generate a print
  print('------------------------------------------------------------------------')
  print(f'Training Dense121 for fold {fold_no} ...')
  
  # batch_size = int(len(img)/100)
  history = model.fit(train_img, train_lab,batch_size=batch,epochs=S,callbacks=[reduce_lr, mc,es],verbose=0,validation_data=(valid_img,valid_lab))
  # history = model.fit(train_img, train_lab,batch_size=batch,epochs=S,verbose=0,validation_data=(valid_img,valid_lab))
  H.append(history)
  top1,top2,top3=Top_K_accuracy_for_k_fold(model,test_img,test_lab,3,history)
  
  #model_copy = model
  
  # Generate generalization metrics
  a,b=model.evaluate(test_img,test_lab)
  model_path = '/content/drive/My Drive/volume_2021_model/'+str(fold_no)
  model.save(model_path)
  ##
  #model_path_1 = '/content/drive/My Drive/true/'+str(fold_no)
  #model_json = model_copy.to_json()
  #with open(model_path_1 + "model.json","w") as json_file :
  #  json_file.write(model_json)

  #model_copy.save_weights(model_path_1 + '/model.h5')
  #model_copy.save(model_path_1 + '/model_1.h5')
  print("Saved model to disk")
  # print(f'Training for fold Dense121{fold_no} ...')
  print(f'Dense121 Evaluate Score for fold {fold_no}: [{model.metrics_names[0]} of {round(a,3)}], [{model.metrics_names[1]} of {round(b,3)*100}%]')
  d121_acc_per_fold.append(b)
  d121_loss_per_fold.append(a)
  d121_t1.append(top1)
  d121_t2.append(top2)
  d121_t3.append(top3)

  # Increase fold number
  fold_no = fold_no + 1


# == Provide average scores ==
print('--------------------------Dense Net 121-------------------------------------')
print('Score per fold')
for i in range(0, len(d121_acc_per_fold)):
  print('------------------------------------------------------------------------')
  print(f'> Fold {i+1} - Loss: {round(d121_loss_per_fold[i],3)} - Accuracy: {round(d121_acc_per_fold[i],3)*100}%')
print('------------------------------------------------------------------------')
print('Average scores for all folds:')
print(f'> Accuracy: {round(np.mean(d121_acc_per_fold),3)} (+- {round(np.std(d121_acc_per_fold),2)})')
print(f'> Loss: {round(np.mean(d121_loss_per_fold),2)}')
print(f'> top 1 error : {round(np.mean(d121_t1),2)}%')
print(f'> top 2 error : {round(np.mean(d121_t2),2)}%')
print(f'> top 3 error : {round(np.mean(d121_t3),2)}%')



l0=np.asarray(H[0].history['loss'])
l1=np.asarray(H[1].history['loss'])
l2=np.asarray(H[2].history['loss'])
l3=np.asarray(H[3].history['loss'])
l4=np.asarray(H[4].history['loss'])

v_l0=np.asarray(H[0].history['val_loss'])
v_l1=np.asarray(H[1].history['val_loss'])
v_l2=np.asarray(H[2].history['val_loss'])
v_l3=np.asarray(H[3].history['val_loss'])
v_l4=np.asarray(H[4].history['val_loss'])



a0=np.asarray(H[0].history['accuracy'])
a1=np.asarray(H[1].history['accuracy'])
a2=np.asarray(H[2].history['accuracy'])
a3=np.asarray(H[3].history['accuracy'])
a4=np.asarray(H[4].history['accuracy'])

v_a0=np.asarray(H[0].history['val_accuracy'])
v_a1=np.asarray(H[1].history['val_accuracy'])
v_a2=np.asarray(H[2].history['val_accuracy'])
v_a3=np.asarray(H[3].history['val_accuracy'])
v_a4=np.asarray(H[4].history['val_accuracy'])


plt.plot((l0+l1+l2+l3+l4)/5)
plt.plot((v_l0+v_l1+v_l2+v_l3+v_l4)/5)
plt.title('Dense type model with raw data  loss',color='w')
plt.ylabel('loss',color='w')
plt.xlabel('epoch',color='w')
plt.legend(['train', 'valid'], loc='upper left')
plt.show()

plt.plot((a0+a1+a2+a3+a4)/5)
plt.plot((v_a0+v_a1+v_a2+v_a3+v_a4)/5)
plt.title('Dense type model with raw data accuracy',color='w')
plt.ylabel('accuracy',color='w')
plt.xlabel('epoch',color='w')
plt.legend(['train', 'valid'], loc='upper left')
plt.show()

from keras.models import load_model
#import keras

#model_1 = load_model('/content/drive/MyDrive/volume_2021_model/1')
#model_1.summary()
#model_2 = load_model('/content/drive/MyDrive/volume_2021_model/2')
#model_2.summary()
model_3 = load_model('/content/drive/MyDrive/volume_2021_model/3')
#model_3.summary()
#model_4 = load_model('/content/drive/MyDrive/volume_2021_model/4')
#model_4.summary()
#model_5 = load_model('/content/drive/MyDrive/volume_2021_model/5')
#model_5.summary()

#model_1 = model.load_weights('/content/drive/My Drive/model.h5')
#model_1.summary()

import numpy as np
from PIL import Image
fpaths = '/content/drive/MyDrive/test_img/test1.jpg'
m = 224
n = 224
label = 'vol2'
img_array = []
lab_array = []

# print(fname)  
img = Image.open(fpaths).convert('RGB')
print(img.size)
img = img.resize((m, n))
print(img.size)
img_array.append(np.asarray(img))

#print(img_array.shape)
lab_array.append(label)

images = np.array(img_array)
labels = np.array(lab_array)

images = images.astype(np.float32) /255.

print(images.shape)
images.resize(1,224,224)
print(images.shape)
#pre_ans1 = model_1.predict(images).argmax()
#pre_ans2 = model_2.predict(images).argmax()
#pre_ans3 = model_3.predict(images).argmax()
#pre_ans4 = model_4.predict(images).argmax()
#pre_ans5 = model_5.predict(images).argmax()

model_list1 = model_3.predict(images)

#print(images.shape)
#print(label)
print("------------------------------")

#print(model_list1)
#print(pre_ans1)
print("v0 :",model_list1[0][0])
print("v1 :",model_list1[0][1])
print("v2 :",model_list1[0][2])
#print(model_2.predict(images))
#print(pre_ans2)
#print(model_3.predict(images))
#print(pre_ans3)
#print(model_4.predict(images))
#print(pre_ans4)
#print(model_5.predict(images))
#print(pre_ans5)



# model_1.predict(test)

import os
from PIL import Image
import cv2 as cv
from matplotlib import image
from matplotlib import pyplot as plt
import torch
import pandas as pd
from torch.utils.data import Dataset, DataLoader
import numpy as np
import torchvision
from torchvision import transforms
import random
from glob import glob
import gc
from sklearn.utils import shuffle
from numpy.random import seed
from sklearn.model_selection import KFold
from keras.models import load_model

#print(images.shape)

test0_list = glob('/content/drive/MyDrive/test_img/test_volume0/*.jpg')
test1_list = glob('/content/drive/MyDrive/test_img/test_volume1/*.jpg')
test2_list = glob('/content/drive/MyDrive/test_img/test_volume2/*.jpg')

index_0 = []
index_1 = []
index_2 = []
'''
balance = min(len(test0_list),len(test1_list),len(test2_list))
# balance = 3
for i in range((balance)):
  # if i not in index_0
  index_0.append(test0_list[random.randint(0,len(test0_list)-1)])
  index_1.append(test1_list[random.randint(0,len(test1_list)-1)])
  index_2.append(test2_list[random.randint(0,len(test2_list)-1)])
'''
img_volume0,lab_volume0 = img_to_np(test0_list,0,224,224)
img_volume0 = img_volume0.astype(np.float32)/ 255.

img_volume1,lab_volume1 = img_to_np(test1_list,1,224,224)
img_volume1 = img_volume1.astype(np.float32)/ 255.

img_volume2,lab_volume2 = img_to_np(test2_list,2,224,224)
img_volume2 = img_volume2.astype(np.float32)/ 255.

print(len(img_volume0))
print(len(img_volume1))
print(len(img_volume2))

#print(img_volume0.shape)

#img_volume0[0].reshape(1,224,224,3).shape

for i in range(len(img_volume0)):
  images = img_volume0[i].reshape(1,224,224,3)
  model_list1 = model_3.predict(images)
  pre_ans1 = model_list1.argmax()
  print(model_list1)
  print("예측값 :",pre_ans1)
  print("----------------------------",i+1,"번")

for i in range(len(img_volume1)):
  images = img_volume1[i].reshape(1,224,224,3)
  model_list1 = model_2.predict(images)
  pre_ans1 = model_list1.argmax()
  print(model_list1)
  print("예측값 :",pre_ans1)
  print("----------------------------",i+1,"번")


for i in range(len(img_volume2)):
  images = img_volume2[i].reshape(1,224,224,3)
  model_list1 = model_2.predict(images)
  pre_ans1 = model_list1.argmax()
  print(model_list1)
  print("예측값 :",pre_ans1)
  print("----------------------------",i+1,"번")