{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seqtest.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlLARtU764KH",
        "outputId": "f1ff93af-c9db-409c-b733-07a6790140b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 샘플의 개수 : 192341\n",
            "                           src                                tar\n",
            "43664    Please pay attention.  Faites attention s'il vous plait.\n",
            "48455   I don't need anything.            Je n'ai besoin de rien.\n",
            "58310  They admire each other.       Ils s'admirent l'un l'autre.\n",
            "51590   This is a joke, right?           C'est une blague, hein ?\n",
            "15349         Someone coughed.                Quelqu'un a toussé.\n",
            "14540         I'm not perfect.            Je ne suis pas parfait.\n",
            "27227       You can wait here.              Tu peux attendre ici.\n",
            "11099          It's redundant.                   C'est redondant.\n",
            "28378      Give me one minute.             Donnez-moi une minute.\n",
            "19041        I'm homeschooled.    Je suis scolarisée à la maison.\n",
            "simbol 추가\n",
            "                          src                              tar\n",
            "20344       They'll kill you.            \\t Ils te tueront. \\n\n",
            "2052              I work out.     \\t Je fais de l'exercice. \\n\n",
            "51767  Tom continued writing.   \\t Tom a continué à écrire. \\n\n",
            "42690   I'm highly motivated.   \\t Vous m'avez bien motivé. \\n\n",
            "4988            I said maybe.        \\t J'ai dit peut-être. \\n\n",
            "37992    Three weeks went by.  \\t Trois semaines ont passé. \\n\n",
            "50778  She is afraid of cats.     \\t Elle a peur des chats. \\n\n",
            "31719     This is incredible.          \\t C'est incroyable. \\n\n",
            "21                       Who?                      \\t Qui ? \\n\n",
            "28907     I answered for him.      \\t Je répondis pour lui. \\n\n",
            "source 문장의 char 집합 : 80\n",
            "target 문장의 char 집합 : 105\n",
            "['W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "['T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w']\n",
            "{' ': 1, '!': 2, '\"': 3, '$': 4, '%': 5, '&': 6, \"'\": 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, '?': 23, 'A': 24, 'B': 25, 'C': 26, 'D': 27, 'E': 28, 'F': 29, 'G': 30, 'H': 31, 'I': 32, 'J': 33, 'K': 34, 'L': 35, 'M': 36, 'N': 37, 'O': 38, 'P': 39, 'Q': 40, 'R': 41, 'S': 42, 'T': 43, 'U': 44, 'V': 45, 'W': 46, 'X': 47, 'Y': 48, 'Z': 49, 'a': 50, 'b': 51, 'c': 52, 'd': 53, 'e': 54, 'f': 55, 'g': 56, 'h': 57, 'i': 58, 'j': 59, 'k': 60, 'l': 61, 'm': 62, 'n': 63, 'o': 64, 'p': 65, 'q': 66, 'r': 67, 's': 68, 't': 69, 'u': 70, 'v': 71, 'w': 72, 'x': 73, 'y': 74, 'z': 75, '°': 76, 'é': 77, '’': 78, '€': 79}\n",
            "{'\\t': 1, '\\n': 2, ' ': 3, '!': 4, '\"': 5, '$': 6, '%': 7, '&': 8, \"'\": 9, '(': 10, ')': 11, ',': 12, '-': 13, '.': 14, '0': 15, '1': 16, '2': 17, '3': 18, '4': 19, '5': 20, '6': 21, '7': 22, '8': 23, '9': 24, ':': 25, '?': 26, 'A': 27, 'B': 28, 'C': 29, 'D': 30, 'E': 31, 'F': 32, 'G': 33, 'H': 34, 'I': 35, 'J': 36, 'K': 37, 'L': 38, 'M': 39, 'N': 40, 'O': 41, 'P': 42, 'Q': 43, 'R': 44, 'S': 45, 'T': 46, 'U': 47, 'V': 48, 'W': 49, 'X': 50, 'Y': 51, 'Z': 52, 'a': 53, 'b': 54, 'c': 55, 'd': 56, 'e': 57, 'f': 58, 'g': 59, 'h': 60, 'i': 61, 'j': 62, 'k': 63, 'l': 64, 'm': 65, 'n': 66, 'o': 67, 'p': 68, 'q': 69, 'r': 70, 's': 71, 't': 72, 'u': 73, 'v': 74, 'w': 75, 'x': 76, 'y': 77, 'z': 78, '\\xa0': 79, '«': 80, '»': 81, 'À': 82, 'Ç': 83, 'É': 84, 'Ê': 85, 'Ô': 86, 'à': 87, 'â': 88, 'ç': 89, 'è': 90, 'é': 91, 'ê': 92, 'ë': 93, 'î': 94, 'ï': 95, 'ô': 96, 'ù': 97, 'û': 98, 'œ': 99, '\\u2009': 100, '\\u200b': 101, '‘': 102, '’': 103, '\\u202f': 104}\n",
            "source 문장의 정수 인코딩 : [[30, 64, 10], [30, 64, 10], [30, 64, 10], [31, 58, 10], [31, 58, 10]]\n",
            "target 문장의 정수 인코딩 : [[1, 3, 48, 53, 3, 4, 3, 2], [1, 3, 39, 53, 70, 55, 60, 57, 14, 3, 2], [1, 3, 28, 67, 73, 59, 57, 3, 4, 3, 2], [1, 3, 45, 53, 64, 73, 72, 3, 4, 3, 2], [1, 3, 45, 53, 64, 73, 72, 14, 3, 2]]\n",
            "target 문장 레이블의 정수 인코딩 : [[3, 48, 53, 3, 4, 3, 2], [3, 39, 53, 70, 55, 60, 57, 14, 3, 2], [3, 28, 67, 73, 59, 57, 3, 4, 3, 2], [3, 45, 53, 64, 73, 72, 3, 4, 3, 2], [3, 45, 53, 64, 73, 72, 14, 3, 2]]\n",
            "source 문장의 최대 길이 : 23\n",
            "target 문장의 최대 길이 : 76\n",
            "Epoch 1/40\n",
            "750/750 [==============================] - 41s 45ms/step - loss: 0.7410 - val_loss: 0.6653\n",
            "Epoch 2/40\n",
            "750/750 [==============================] - 32s 43ms/step - loss: 0.4570 - val_loss: 0.5329\n",
            "Epoch 3/40\n",
            "750/750 [==============================] - 32s 43ms/step - loss: 0.3821 - val_loss: 0.4711\n",
            "Epoch 4/40\n",
            "750/750 [==============================] - 33s 44ms/step - loss: 0.3403 - val_loss: 0.4337\n",
            "Epoch 5/40\n",
            "750/750 [==============================] - 32s 42ms/step - loss: 0.3123 - val_loss: 0.4127\n",
            "Epoch 6/40\n",
            "750/750 [==============================] - 32s 43ms/step - loss: 0.2927 - val_loss: 0.3942\n",
            "Epoch 7/40\n",
            "750/750 [==============================] - 32s 42ms/step - loss: 0.2757 - val_loss: 0.3846\n",
            "Epoch 8/40\n",
            "750/750 [==============================] - 32s 43ms/step - loss: 0.2625 - val_loss: 0.3732\n",
            "Epoch 9/40\n",
            "750/750 [==============================] - 32s 43ms/step - loss: 0.2516 - val_loss: 0.3660\n",
            "Epoch 10/40\n",
            "750/750 [==============================] - 32s 43ms/step - loss: 0.2422 - val_loss: 0.3596\n",
            "Epoch 11/40\n",
            "750/750 [==============================] - 32s 42ms/step - loss: 0.2337 - val_loss: 0.3585\n",
            "Epoch 12/40\n",
            "750/750 [==============================] - 32s 42ms/step - loss: 0.2264 - val_loss: 0.3545\n",
            "Epoch 13/40\n",
            "750/750 [==============================] - 32s 43ms/step - loss: 0.2196 - val_loss: 0.3517\n",
            "Epoch 14/40\n",
            "750/750 [==============================] - 32s 43ms/step - loss: 0.2134 - val_loss: 0.3507\n",
            "Epoch 15/40\n",
            "750/750 [==============================] - 32s 42ms/step - loss: 0.2078 - val_loss: 0.3504\n",
            "Epoch 16/40\n",
            "750/750 [==============================] - 32s 42ms/step - loss: 0.2026 - val_loss: 0.3506\n",
            "Epoch 17/40\n",
            "750/750 [==============================] - 32s 43ms/step - loss: 0.1978 - val_loss: 0.3500\n",
            "Epoch 18/40\n",
            "750/750 [==============================] - 32s 43ms/step - loss: 0.1934 - val_loss: 0.3515\n",
            "Epoch 19/40\n",
            "750/750 [==============================] - 33s 43ms/step - loss: 0.1891 - val_loss: 0.3520\n",
            "Epoch 20/40\n",
            "750/750 [==============================] - 32s 42ms/step - loss: 0.1851 - val_loss: 0.3519\n",
            "Epoch 21/40\n",
            "750/750 [==============================] - 32s 43ms/step - loss: 0.1815 - val_loss: 0.3552\n",
            "Epoch 22/40\n",
            "750/750 [==============================] - 34s 45ms/step - loss: 0.1780 - val_loss: 0.3568\n",
            "Epoch 23/40\n",
            "750/750 [==============================] - 34s 46ms/step - loss: 0.1746 - val_loss: 0.3571\n",
            "Epoch 24/40\n",
            "750/750 [==============================] - 32s 43ms/step - loss: 0.1713 - val_loss: 0.3593\n",
            "Epoch 25/40\n",
            "750/750 [==============================] - 32s 43ms/step - loss: 0.1683 - val_loss: 0.3614\n",
            "Epoch 26/40\n",
            "750/750 [==============================] - 32s 42ms/step - loss: 0.1655 - val_loss: 0.3622\n",
            "Epoch 27/40\n",
            "750/750 [==============================] - 32s 43ms/step - loss: 0.1627 - val_loss: 0.3659\n",
            "Epoch 28/40\n",
            "750/750 [==============================] - 32s 42ms/step - loss: 0.1601 - val_loss: 0.3674\n",
            "Epoch 29/40\n",
            "750/750 [==============================] - 32s 43ms/step - loss: 0.1576 - val_loss: 0.3698\n",
            "Epoch 30/40\n",
            "750/750 [==============================] - 32s 43ms/step - loss: 0.1550 - val_loss: 0.3730\n",
            "Epoch 31/40\n",
            "750/750 [==============================] - 32s 43ms/step - loss: 0.1528 - val_loss: 0.3753\n",
            "Epoch 32/40\n",
            "750/750 [==============================] - 32s 42ms/step - loss: 0.1506 - val_loss: 0.3773\n",
            "Epoch 33/40\n",
            "750/750 [==============================] - 32s 42ms/step - loss: 0.1484 - val_loss: 0.3801\n",
            "Epoch 34/40\n",
            "750/750 [==============================] - 32s 43ms/step - loss: 0.1464 - val_loss: 0.3817\n",
            "Epoch 35/40\n",
            "750/750 [==============================] - 32s 43ms/step - loss: 0.1444 - val_loss: 0.3872\n",
            "Epoch 36/40\n",
            "750/750 [==============================] - 32s 42ms/step - loss: 0.1426 - val_loss: 0.3890\n",
            "Epoch 37/40\n",
            "750/750 [==============================] - 32s 43ms/step - loss: 0.1407 - val_loss: 0.3926\n",
            "Epoch 38/40\n",
            "750/750 [==============================] - 32s 43ms/step - loss: 0.1390 - val_loss: 0.3959\n",
            "Epoch 39/40\n",
            "750/750 [==============================] - 32s 43ms/step - loss: 0.1373 - val_loss: 0.3971\n",
            "Epoch 40/40\n",
            "750/750 [==============================] - 32s 43ms/step - loss: 0.1356 - val_loss: 0.4008\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f04860c2ad0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import urllib3\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "#http = urllib3.PoolManager()\n",
        "#url ='http://www.manythings.org/anki/fra-eng.zip'\n",
        "filename = 'fra-eng.zip'\n",
        "path = os.getcwd()\n",
        "zipfilename = os.path.join(path, filename)\n",
        "#with http.request('GET', url, preload_content=False) as r, open(zipfilename, 'wb') as out_file:       \n",
        "#    shutil.copyfileobj(r, out_file)\n",
        "\n",
        "#with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n",
        "#    zip_ref.extractall(path)\n",
        "\n",
        "lines = pd.read_csv('/content/drive/MyDrive/fra-eng/fra.txt', names=['src', 'tar', 'lic'], sep='\\t')\n",
        "del lines['lic']\n",
        "print('전체 샘플의 개수 :',len(lines))\n",
        "\n",
        "lines = lines.loc[:, 'src':'tar']\n",
        "lines = lines[0:60000] # 6만개만 저장\n",
        "print(lines.sample(10))\n",
        "\n",
        "# target data에 시작<sos>과 종료<eos>의 심볼을 넣어주어야 한다.\n",
        "# <sos> : \"\\t\"  <eos> : \"\\n\"\n",
        "lines.tar = lines.tar.apply(lambda x : '\\t '+ x + ' \\n')\n",
        "print(\"simbol 추가\")\n",
        "print(lines.sample(10))\n",
        "\n",
        "# 토큰의 단위를 단어가 아닌 문자로 \n",
        "# 문자 집합 구축\n",
        "src_vocab = set()\n",
        "for line in lines.src: # 1줄씩 읽음\n",
        "    for char in line: # 1개의 문자씩 읽음\n",
        "        src_vocab.add(char)\n",
        "\n",
        "tar_vocab = set()\n",
        "for line in lines.tar:\n",
        "    for char in line:\n",
        "        tar_vocab.add(char)\n",
        "\n",
        "src_vocab_size = len(src_vocab)+1\n",
        "tar_vocab_size = len(tar_vocab)+1\n",
        "print('source 문장의 char 집합 :',src_vocab_size)\n",
        "print('target 문장의 char 집합 :',tar_vocab_size)\n",
        "\n",
        "# set때문에 바로 인덱스 사용이 안된다. 그래서 정렬하여 순서를 정해준 뒤 인덱스 사용\n",
        "src_vocab = sorted(list(src_vocab))\n",
        "tar_vocab = sorted(list(tar_vocab))\n",
        "print(src_vocab[45:75])\n",
        "print(tar_vocab[45:75])\n",
        "\n",
        "# 인덱스 부여\n",
        "src_to_index = dict([(word, i+1) for i, word in enumerate(src_vocab)])\n",
        "tar_to_index = dict([(word, i+1) for i, word in enumerate(tar_vocab)])\n",
        "print(src_to_index)\n",
        "print(tar_to_index)\n",
        "\n",
        "# 훈련 데이터 정수 인코딩\n",
        "# 인코더 입력되는 영어 문장 먼저 수행\n",
        "encoder_input = []\n",
        "\n",
        "# 1개의 문장\n",
        "for line in lines.src:\n",
        "  encoded_line = []\n",
        "  # 각 줄에서 1개의 char\n",
        "  for char in line:\n",
        "    # 각 char을 정수로 변환\n",
        "    encoded_line.append(src_to_index[char])\n",
        "  encoder_input.append(encoded_line)\n",
        "print('source 문장의 정수 인코딩 :',encoder_input[:5])\n",
        "\n",
        "# 디코더의 입력이 될 프랑스어 데이터 수행\n",
        "decoder_input = []\n",
        "for line in lines.tar:\n",
        "  encoded_line = []\n",
        "  for char in line:\n",
        "    encoded_line.append(tar_to_index[char])\n",
        "  decoder_input.append(encoded_line)\n",
        "print('target 문장의 정수 인코딩 :',decoder_input[:5])\n",
        "\n",
        "# 예측값과 비교하기 위한 실제값이 필요하다.\n",
        "# 실제 값에는 시작 심볼에 해당하는 <sos>가 있을 필요가 없다.\n",
        "# \"\\t\" 제거\n",
        "decoder_target = []\n",
        "for line in lines.tar:\n",
        "  timestep = 0\n",
        "  encoded_line = []\n",
        "  for char in line:\n",
        "    if timestep > 0:\n",
        "      encoded_line.append(tar_to_index[char])\n",
        "    timestep = timestep + 1\n",
        "  decoder_target.append(encoded_line)\n",
        "print('target 문장 레이블의 정수 인코딩 :',decoder_target[:5])  # 문장 앞 숫자 1 제거됨\n",
        "\n",
        "# 문장 최대 길이에 맞추어 패딩 수행\n",
        "max_src_len = max([len(line) for line in lines.src])\n",
        "max_tar_len = max([len(line) for line in lines.tar])\n",
        "print('source 문장의 최대 길이 :',max_src_len)\n",
        "print('target 문장의 최대 길이 :',max_tar_len)\n",
        "\n",
        "# 영어 데이터는 영어(23), 프랑스어 데이터는 프랑스어 끼리 패딩(76)\n",
        "encoder_input = pad_sequences(encoder_input, maxlen=max_src_len, padding='post')\n",
        "decoder_input = pad_sequences(decoder_input, maxlen=max_tar_len, padding='post')\n",
        "decoder_target = pad_sequences(decoder_target, maxlen=max_tar_len, padding='post')\n",
        "\n",
        "# 모든 값에 대해 one-hot incoding\n",
        "encoder_input = to_categorical(encoder_input)\n",
        "decoder_input = to_categorical(decoder_input)\n",
        "decoder_target = to_categorical(decoder_target)\n",
        "'''전처리 완료'''\n",
        "\n",
        "# 훈련\n",
        "# Teacher forcing\n",
        "# 이전 시점의 디코더 셀의 출력을 현재 시점의 디코더 셀의 입력으로 넣어주지 않고\n",
        "# 이전 시점의 실제값을 현재 시점의 디코더 셀의 입력값으로 넣는 방법\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "\n",
        "encoder_inputs = Input(shape=(None, src_vocab_size))\n",
        "encoder_lstm = LSTM(units=256, return_state=True)   # LSTM 은닉 상태 크기 256, 인코더의 내부 상태를 디코더로 넘겨주기 위해 =True\n",
        "\n",
        "# encoder_outputs은 여기서는 불필요\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
        "# state_h : 은닉 상태, state_c : 셀 상태\n",
        "\n",
        "# LSTM은 바닐라 RNN과는 달리 상태가 두 개. 은닉 상태와 셀 상태.\n",
        "encoder_states = [state_h, state_c] # encoder_state : context vector\n",
        "\n",
        "decoder_inputs = Input(shape=(None, tar_vocab_size))\n",
        "decoder_lstm = LSTM(units=256, return_sequences=True, return_state=True)\n",
        "\n",
        "# 디코더에게 인코더의 은닉 상태, 셀 상태를 전달.\n",
        "decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "# 훈련과정에서는 디코더의 은닉상태, 셀 상태를 사용하진 않는다.\n",
        "\n",
        "decoder_softmax_layer = Dense(tar_vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")\n",
        "\n",
        "model.fit(x=[encoder_input, decoder_input], y=decoder_target, batch_size=64, epochs=40, validation_split=0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)\n",
        "\n",
        "# 이전 시점의 상태들을 저장하는 텐서\n",
        "decoder_state_input_h = Input(shape=(256,))\n",
        "decoder_state_input_c = Input(shape=(256,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용.\n",
        "# 뒤의 함수 decode_sequence()에 동작을 구현 예정\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "\n",
        "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태를 버리지 않음.\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
        "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)\n",
        "\n",
        "index_to_src = dict((i, char) for char, i in src_to_index.items())\n",
        "index_to_tar = dict((i, char) for char, i in tar_to_index.items())\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "  # 입력으로부터 인코더의 상태를 얻음\n",
        "  states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "  # <SOS>에 해당하는 원-핫 벡터 생성\n",
        "  target_seq = np.zeros((1, 1, tar_vocab_size))\n",
        "  target_seq[0, 0, tar_to_index['\\t']] = 1.\n",
        "\n",
        "  stop_condition = False\n",
        "  decoded_sentence = \"\"\n",
        "\n",
        "  # stop_condition이 True가 될 때까지 루프 반복\n",
        "  while not stop_condition:\n",
        "    # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
        "    output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "    # 예측 결과를 문자로 변환\n",
        "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "    sampled_char = index_to_tar[sampled_token_index]\n",
        "\n",
        "    # 현재 시점의 예측 문자를 예측 문장에 추가\n",
        "    decoded_sentence += sampled_char\n",
        "\n",
        "    # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
        "    if (sampled_char == '\\n' or\n",
        "        len(decoded_sentence) > max_tar_len):\n",
        "        stop_condition = True\n",
        "\n",
        "    # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
        "    target_seq = np.zeros((1, 1, tar_vocab_size))\n",
        "    target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "    # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
        "    states_value = [h, c]\n",
        "\n",
        "  return decoded_sentence\n",
        "\n",
        "for seq_index in [3,50,100,300,1001]: # 입력 문장의 인덱스\n",
        "  input_seq = encoder_input[seq_index:seq_index+1]\n",
        "  decoded_sentence = decode_sequence(input_seq)\n",
        "  print(35 * \"-\")\n",
        "  print('입력 문장:', lines.src[seq_index])\n",
        "  print('정답 문장:', lines.tar[seq_index][2:len(lines.tar[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
        "  print('번역 문장:', decoded_sentence[1:len(decoded_sentence)-1]) # '\\n'을 빼고 출력\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tf4lMvnY-ONb",
        "outputId": "04f723bc-5feb-4014-be8f-f7d00df724e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------\n",
            "입력 문장: Hi.\n",
            "정답 문장: Salut ! \n",
            "번역 문장: Salut. \n",
            "-----------------------------------\n",
            "입력 문장: I see.\n",
            "정답 문장: Aha. \n",
            "번역 문장: J'ai tout. \n",
            "-----------------------------------\n",
            "입력 문장: Hug me.\n",
            "정답 문장: Serrez-moi dans vos bras ! \n",
            "번역 문장: Serre-moi dans tes bras ! \n",
            "-----------------------------------\n",
            "입력 문장: Help me.\n",
            "정답 문장: Aidez-moi. \n",
            "번역 문장: Aide-moi. \n",
            "-----------------------------------\n",
            "입력 문장: I am sure.\n",
            "정답 문장: Je suis sûr. \n",
            "번역 문장: Je suis de la maison. \n"
          ]
        }
      ]
    }
  ]
}